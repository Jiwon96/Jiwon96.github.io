---
layout: post
title: "신경망 역전파 알고리즘의 행렬 미분 과정 완벽 해부"
date: 2024-12-01 10:00:00 +0900
categories: [AI]
tags: [neural-network, backpropagation, matrix-calculus, deep-learning]
---

# 신경망 역전파 알고리즘의 행렬 미분 과정 완벽 해부

## 들어가며

신경망의 역전파(Backpropagation) 알고리즘은 딥러닝의 핵심 개념 중 하나입니다. 특히 `dZ[1] = W[2]ᵀ * dZ[2] * g'[1](Z[1])`와 같은 행렬 미분 공식이 어떻게 도출되는지 이해하는 것은 매우 중요합니다. 이번 포스트에서는 이 과정을 수학적 근거와 함께 단계별로 해부해보겠습니다.

## 1. 단층 신경망 구조 이해

### 순전파(Forward Propagation) 과정
```
입력층 → 은닉층 → 출력층
X → Z[1] = W[1]X + b[1] → A[1] = g[1](Z[1]) → Z[2] = W[2]A[1] + b[2] → A[2] = g[2](Z[2])
```

여기서:
- `X`: 입력 데이터 (n_x × m)
- `W[1]`: 첫 번째 가중치 행렬 (n_h × n_x)
- `b[1]`: 첫 번째 편향 벡터 (n_h × 1)
- `A[1]`: 은닉층 활성화 값 (n_h × m)
- `W[2]`: 두 번째 가중치 행렬 (n_y × n_h)
- `b[2]`: 두 번째 편향 벡터 (n_y × 1)
- `A[2]`: 출력층 활성화 값 (n_y × m)

## 2. 연쇄 법칙(Chain Rule)의 핵심

역전파의 핵심은 **연쇄 법칙**입니다. 손실 함수 L에 대한 Z[1]의 기울기를 구하려면:

```
∂L/∂Z[1] = (∂L/∂Z[2]) × (∂Z[2]/∂A[1]) × (∂A[1]/∂Z[1])
```

이를 행렬 표기법으로 나타내면:
```
dZ[1] = dZ[2] × (∂Z[2]/∂A[1]) × (∂A[1]/∂Z[1])
```

## 3. 각 편미분 항목별 계산

### 3.1. ∂Z[2]/∂A[1] 계산

`Z[2] = W[2]A[1] + b[2]`이므로:

```
∂Z[2]/∂A[1] = W[2]
```

### 3.2. ∂A[1]/∂Z[1] 계산

`A[1] = g[1](Z[1])`이므로:

```
∂A[1]/∂Z[1] = g'[1](Z[1])
```

### 3.3. 행렬 차원 분석

- `dZ[2]`: (n_y × m)
- `W[2]`: (n_y × n_h)  
- `g'[1](Z[1])`: (n_h × m)
- `dZ[1]`: (n_h × m) ← 목표 차원

## 4. 행렬 곱셈 순서의 수학적 근거

### 4.1. 벡터화되지 않은 형태에서의 이해

단일 샘플에 대해 생각해보면:
```
∂L/∂z[1]ᵢ = Σⱼ (∂L/∂z[2]ⱼ × ∂z[2]ⱼ/∂a[1]ᵢ × ∂a[1]ᵢ/∂z[1]ᵢ)
           = Σⱼ (dz[2]ⱼ × w[2]ⱼᵢ × g'[1](z[1]ᵢ))
           = g'[1](z[1]ᵢ) × Σⱼ (w[2]ⱼᵢ × dz[2]ⱼ)
```

### 4.2. 행렬 형태로의 확장

위 수식을 행렬 형태로 표현하면:
```
dZ[1] = (W[2]ᵀ × dZ[2]) ⊙ g'[1](Z[1])
```

여기서 `⊙`는 요소별 곱셈(element-wise multiplication)을 의미합니다.

## 5. 전치행렬 사용의 수학적 증명

### 5.1. 기본 설정 및 표기법

먼저 각 변수의 차원을 명확히 정의하겠습니다:
- `Z[1]`: (n_h × m) - 은닉층의 선형 결합값
- `A[1]`: (n_h × m) - 은닉층의 활성화값
- `Z[2]`: (n_y × m) - 출력층의 선형 결합값
- `W[2]`: (n_y × n_h) - 출력층 가중치 행렬
- `dZ[2]`: (n_y × m) - 출력층 기울기

### 5.2. 단일 샘플에 대한 미분 전개

단일 샘플 i에 대해 연쇄법칙을 적용하면:

```
∂L/∂z[1]ⱼ^(i) = Σₖ (∂L/∂z[2]ₖ^(i)) × (∂z[2]ₖ^(i)/∂a[1]ⱼ^(i)) × (∂a[1]ⱼ^(i)/∂z[1]ⱼ^(i))
```

여기서:
- j는 은닉층 뉴런 인덱스 (1 ≤ j ≤ n_h)
- k는 출력층 뉴런 인덱스 (1 ≤ k ≤ n_y)
- i는 샘플 인덱스 (1 ≤ i ≤ m)

### 5.3. 각 편미분 항목 계산

**1단계**: `∂z[2]ₖ^(i)/∂a[1]ⱼ^(i)` 계산

`z[2]ₖ^(i) = Σₗ w[2]ₖₗ × a[1]ₗ^(i) + b[2]ₖ`이므로:

```
∂z[2]ₖ^(i)/∂a[1]ⱼ^(i) = w[2]ₖⱼ
```

**2단계**: `∂a[1]ⱼ^(i)/∂z[1]ⱼ^(i)` 계산

```
∂a[1]ⱼ^(i)/∂z[1]ⱼ^(i) = g'[1](z[1]ⱼ^(i))
```

### 5.4. 전체 미분 공식 도출

위 결과를 종합하면:

```
∂L/∂z[1]ⱼ^(i) = Σₖ (∂L/∂z[2]ₖ^(i)) × w[2]ₖⱼ × g'[1](z[1]ⱼ^(i))
                = g'[1](z[1]ⱼ^(i)) × Σₖ w[2]ₖⱼ × (∂L/∂z[2]ₖ^(i))
```

### 5.5. 행렬 형태로의 변환 증명

**핵심**: `Σₖ w[2]ₖⱼ × (∂L/∂z[2]ₖ^(i))`를 행렬 곱셈으로 표현

이 합은 다음과 같이 쓸 수 있습니다:

```
[w[2]₁ⱼ, w[2]₂ⱼ, ..., w[2]ₙᵧⱼ] × [∂L/∂z[2]₁^(i), ∂L/∂z[2]₂^(i), ..., ∂L/∂z[2]ₙᵧ^(i)]ᵀ
```

이것이 바로 **W[2]의 j번째 열**과 **dZ[2]의 i번째 열**의 내적입니다!

### 5.6. 전치행렬의 필요성 증명

**원래 W[2] 행렬의 구조:**
```
W[2] = [w[2]₁₁  w[2]₁₂  ...  w[2]₁ₙₕ]
       [w[2]₂₁  w[2]₂₂  ...  w[2]₂ₙₕ]
       [  ⋮       ⋮     ⋱     ⋮    ]
       [w[2]ₙᵧ₁  w[2]ₙᵧ₂  ...  w[2]ₙᵧₙₕ]
```

우리가 필요한 것은 j번째 열 벡터 `[w[2]₁ⱼ, w[2]₂ⱼ, ..., w[2]ₙᵧⱼ]ᵀ`입니다.

**W[2]ᵀ의 구조:**
```
W[2]ᵀ = [w[2]₁₁  w[2]₂₁  ...  w[2]ₙᵧ₁]
        [w[2]₁₂  w[2]₂₂  ...  w[2]ₙᵧ₂]
        [  ⋮       ⋮     ⋱     ⋮    ]
        [w[2]₁ₙₕ  w[2]₂ₙₕ  ...  w[2]ₙᵧₙₕ]
```

이제 `W[2]ᵀ`의 j번째 행이 우리가 원하는 `[w[2]₁ⱼ, w[2]₂ⱼ, ..., w[2]ₙᵧⱼ]`입니다!

### 5.7. 최종 행렬 곱셈 공식

모든 샘플에 대해 벡터화하면:

```
dZ[1] = W[2]ᵀ × dZ[2] ⊙ g'[1](Z[1])
```

**차원 검증:**
```
W[2]ᵀ: (n_h × n_y)
dZ[2]: (n_y × m)
W[2]ᵀ × dZ[2]: (n_h × n_y) × (n_y × m) = (n_h × m)
g'[1](Z[1]): (n_h × m)
최종 dZ[1]: (n_h × m) ✓
```

### 5.8. 구체적 수치 예시로 검증

**설정**: n_h=2, n_y=3, m=1

```
W[2] = [1  2]     W[2]ᵀ = [1  3  5]
       [3  4]              [2  4  6]
       [5  6]

dZ[2] = [0.1]
        [0.2]
        [0.3]

W[2]ᵀ × dZ[2] = [1  3  5] × [0.1] = [1×0.1 + 3×0.2 + 5×0.3] = [2.2]
                [2  4  6]   [0.2]   [2×0.1 + 4×0.2 + 6×0.3]   [2.8]
                            [0.3]
```

**검증 - 개별 계산:**
```
dz[1]₁ = w[2]₁₁×dz[2]₁ + w[2]₂₁×dz[2]₂ + w[2]₃₁×dz[2]₃
       = 1×0.1 + 3×0.2 + 5×0.3 = 2.2 ✓

dz[1]₂ = w[2]₁₂×dz[2]₁ + w[2]₂₂×dz[2]₂ + w[2]₃₂×dz[2]₃
       = 2×0.1 + 4×0.2 + 6×0.3 = 2.8 ✓
```

완벽히 일치합니다!

## 6. 실제 계산 예시

### 6.1. 구체적인 수치 예시

가정:
- n_x = 3, n_h = 4, n_y = 1, m = 2
- W[2] = [[0.5, 0.3, -0.2, 0.7]] (1×4)
- dZ[2] = [[0.1, -0.3]] (1×2)
- g'[1](Z[1]) = [[0.2, 0.1], [0.4, 0.3], [0.1, 0.2], [0.3, 0.4]] (4×2)

계산:
```
W[2]ᵀ = [[0.5], [0.3], [-0.2], [0.7]] (4×1)

W[2]ᵀ × dZ[2] = [[0.5], [0.3], [-0.2], [0.7]] × [[0.1, -0.3]]
                = [[0.05, -0.15], [0.03, -0.09], [-0.02, 0.06], [0.07, -0.21]]

dZ[1] = [[0.05, -0.15], [0.03, -0.09], [-0.02, 0.06], [0.07, -0.21]] ⊙ 
        [[0.2, 0.1], [0.4, 0.3], [0.1, 0.2], [0.3, 0.4]]
      = [[0.01, -0.015], [0.012, -0.027], [-0.002, 0.012], [0.021, -0.084]]
```

## 7. 핵심 포인트 정리

### 7.1. 수학적 근거
1. **연쇄 법칙**: ∂L/∂Z[1] = (∂L/∂Z[2]) × (∂Z[2]/∂A[1]) × (∂A[1]/∂Z[1])
2. **행렬 미분**: ∂Z[2]/∂A[1] = W[2]
3. **활성화 함수 미분**: ∂A[1]/∂Z[1] = g'[1](Z[1])

### 7.2. 실용적 관점
1. **전치 행렬 사용 이유**: 차원 맞춤을 위해
2. **요소별 곱셈**: 각 뉴런별로 독립적인 활성화 함수 적용
3. **벡터화**: 여러 샘플을 동시에 처리

## 마무리

`dZ[1] = W[2]ᵀ × dZ[2] ⊙ g'[1](Z[1])` 공식은 단순해 보이지만, 그 뒤에는 깊은 수학적 원리가 숨어있습니다. 연쇄 법칙을 행렬 형태로 확장하고, 차원을 맞추기 위해 전치 행렬을 사용하며, 각 뉴런의 독립성을 보장하기 위해 요소별 곱셈을 사용하는 것입니다.

이러한 이해를 바탕으로 더 깊은 신경망의 역전파 과정도 쉽게 이해할 수 있을 것입니다. 수학적 근거를 명확히 하면 단순 암기가 아닌 진정한 이해가 가능해집니다.

---

**참고 자료**: DeepLearning.AI Neural Networks Overview
